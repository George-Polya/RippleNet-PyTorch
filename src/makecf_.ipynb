{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "from model import RippleNet\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "import multiprocessing\n",
    "from time import time\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from data_loader import load_data\n",
    "from utils import get_feed_dict, _get_topk_feed_data, _get_user_record, topk_settings\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "args = argparse.Namespace()\n",
    "args.dataset = \"naver-toy\"\n",
    "args.dim=32\n",
    "args.n_hop=2\n",
    "args.kge_weight=0.01\n",
    "args.l2_weight=1e-7\n",
    "args.lr = 0.02\n",
    "args.batch_size = 256\n",
    "args.n_epoch = 2\n",
    "args.n_memory=32\n",
    "args.item_update_mode=\"plus_transform\"\n",
    "args.using_all_hops=True\n",
    "args.use_cuda = True\n",
    "args.show_topk =True\n",
    "args.gpu_id=0\n",
    "args.Ks=[20,40,60,80,100]\n",
    "args.test_flag = \"part\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = args.Ks\n",
    "BATCH_SIZE = args.batch_size\n",
    "batch_test_flag = True\n",
    "\n",
    "cores = multiprocessing.cpu_count() // 2\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading rating file ...\n",
      "splitting dataset ...\n",
      "reading KG file ...\n",
      "constructing knowledge graph ...\n",
      "constructing ripple set ...\n"
     ]
    }
   ],
   "source": [
    "data_info = load_data(args)\n",
    "show_loss = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctr_eval(args, model, data, ripple_set, batch_size):\n",
    "    auc_list = []\n",
    "    f1_list = []\n",
    "    model.eval()\n",
    "    start = 0\n",
    "    while start < data.shape[0]:\n",
    "        labels = data[start:start + args.batch_size, 2]\n",
    "        return_dict = model(*get_feed_dict(args, model, data, ripple_set, start, start + batch_size))\n",
    "        scores = return_dict[\"scores\"]\n",
    "        scores = scores.detach().cpu().numpy()\n",
    "        auc = roc_auc_score(y_true=labels, y_score=scores)\n",
    "        predictions = [1 if i >= 0.5 else 0 for i in scores]\n",
    "        f1 = f1_score(y_true=labels, y_pred=predictions)\n",
    "        auc_list.append(auc)\n",
    "        f1_list.append(f1)\n",
    "        start += args.batch_size\n",
    "    model.train()  \n",
    "    auc = float(np.mean(auc_list))\n",
    "    f1 = float(np.mean(f1_list))\n",
    "    return auc, f1\n",
    "\n",
    "\n",
    "\n",
    "def evaluation(args, model, data, ripple_set, batch_size):\n",
    "    start = 0\n",
    "    auc_list = []\n",
    "    acc_list = []\n",
    "    model.eval()\n",
    "    while start < data.shape[0]:\n",
    "        auc, acc = model.evaluate(*get_feed_dict(args, model, data, ripple_set, start, start + batch_size))\n",
    "        auc_list.append(auc)\n",
    "        acc_list.append(acc)\n",
    "        start += batch_size\n",
    "    model.train()\n",
    "    return float(np.mean(auc_list)), float(np.mean(acc_list))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _show_recall_info(recall_zip):\n",
    "    res = \"\"\n",
    "    for i,j in recall_zip:\n",
    "        res += \"K@%d:%.4f  \"%(i,j)\n",
    "    # logging.info(res)\n",
    "    return res\n",
    "\n",
    "def topk_eval(args, model, train_data, test_data, n_item, ripple_set, batch_size):\n",
    "    user_list, train_record, test_record, item_set, k_list = topk_settings(show_topk=True, \n",
    "                                                                        train_data=train_data, \n",
    "                                                                        test_data=test_data, \n",
    "                                                                        n_item=n_item)\n",
    "    precision_list = {k:[] for k in k_list}\n",
    "    recall_list = {k:[] for k in k_list}\n",
    "    ndcg_list = {k:[] for k in k_list}\n",
    "    \n",
    "    model.eval()\n",
    "    for user in user_list:\n",
    "        test_item_list = list(item_set - train_record[user])\n",
    "        item_score_map = dict()\n",
    "        start = 0\n",
    "\n",
    "        while start + batch_size <= len(test_item_list):\n",
    "            items = test_item_list[start:start+batch_size]\n",
    "            input_data = _get_topk_feed_data(user, items)\n",
    "            return_dict = model(*get_feed_dict(args, model, test_data, ripple_set, 0, batch_size))\n",
    "            scores = return_dict[\"scores\"]\n",
    "            for item, score in zip(items, scores):\n",
    "                item_score_map[item] = score\n",
    "            start += args.batch_size\n",
    "\n",
    "        if start < len(test_item_list):\n",
    "            res_items = test_item_list[start:] + [test_item_list[-1]] * (args.batch_size - len(test_item_list) + start)\n",
    "            input_data = _get_topk_feed_data(user, res_items)\n",
    "            # scores = model(*_get_feed_data(args, input_data, user_triple_set, item_triple_set, 0, args.batch_size))\n",
    "            return_dict = model(*get_feed_dict(args, model, test_data, ripple_set, 0, args.batch_size))\n",
    "            scores = return_dict[\"scores\"]\n",
    "            for item, score in zip(res_items, scores):\n",
    "                item_score_map[item] = score\n",
    "        item_score_pair_sorted = sorted(item_score_map.items(), key=lambda x: x[1], reverse=True)\n",
    "        ground_truth = []\n",
    "        item_sorted = [i[0] for i in item_score_pair_sorted]\n",
    "        \n",
    "        ground_truth = []\n",
    "        for i in item_sorted:\n",
    "            if i in test_item_list:\n",
    "                ground_truth.append(1)\n",
    "            else:\n",
    "                ground_truth.append(0)\n",
    "                \n",
    "        \n",
    "        for k in k_list:\n",
    "            hit_num = len(set(item_sorted[:k]) & set(test_record[user]))\n",
    "            recall_list[k].append(hit_num / len(set(test_record[user])))\n",
    "            ndcg_list[k].append(ndcg_at_k(item_sorted[:k], k, ground_truth=ground_truth))\n",
    "            precision_list[k].append(hit_num / len(set(item_sorted[:k])))\n",
    "    model.train()  \n",
    "    recall = [np.mean(recall_list[k]) for k in k_list]\n",
    "    precision = [np.mean(precision_list[k]) for k in k_list]\n",
    "    ndcg = [np.mean(ndcg_list[k]) for k in k_list]\n",
    "    \n",
    "    return recall, precision, ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_feed_dict, _get_user_record, _get_item_record\n",
    "from helper import early_stopping\n",
    "import heapq\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_user, n_item, n_entity, n_relation, train_data, eval_data, test_data, ripple_set = data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7285"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RippleNet(args, n_entity, n_relation)\n",
    "if args.use_cuda:\n",
    "    model.cuda()\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    args.lr,\n",
    ")\n",
    "\n",
    "user_list, train_record, test_record, item_set, k_list = topk_settings(show_topk=True, train_data=train_data, test_data=test_data, n_item=n_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = f\"./training_log/{args.dataset}/\"\n",
    "if not os.path.exists(log_directory):\n",
    "    os.makedirs(log_directory)\n",
    "with open(log_directory+f\"RippleNet_{args.lr}.txt\",\"w\") as f:\n",
    "    for step in range(args.n_epoch):\n",
    "        # training\n",
    "        np.random.shuffle(train_data)\n",
    "        start = 0\n",
    "        train_s_t = time()\n",
    "        while start < train_data.shape[0]:\n",
    "            return_dict = model(*get_feed_dict(args, model, train_data, ripple_set, start, start + args.batch_size))\n",
    "            loss = return_dict[\"loss\"]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            start += args.batch_size\n",
    "            if show_loss:\n",
    "                print('%.1f%% %.4f' % (start / train_data.shape[0] * 100, loss.item()))\n",
    "        train_e_t = time()\n",
    "        train_auc, train_acc = evaluation(args, model, train_data, ripple_set, args.batch_size)\n",
    "        test_s_t = time()\n",
    "        precision_k, recall_k, ndcg_k = topk_eval(args, model, train_data, test_data, n_item, ripple_set, args.batch_size)\n",
    "        test_e_t = time()\n",
    "\n",
    "        test_res = PrettyTable()\n",
    "        test_res.field_names = [\"Epoch\", \"training time\", \"tesing time\", \"Loss\", \"recall\",  \"precision\", \"ndcg\"]\n",
    "        test_res.add_row(\n",
    "            [step, train_e_t-train_s_t, test_e_t-test_s_t, loss.item(), recall_k, precision_k, ndcg_k]\n",
    "        )\n",
    "        print(test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bdffd226db74f32b1e67dcb50a117797eaf84ad68693423287fabc603b29d02"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('tf2.5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
