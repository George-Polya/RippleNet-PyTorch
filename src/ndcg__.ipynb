{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "from model import RippleNet\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "import multiprocessing\n",
    "from time import time\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from data_loader import load_data\n",
    "from utils import get_feed_dict, _get_topk_feed_data, _get_user_record\n",
    "import os\n",
    "import random\n",
    "seed = 2020\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "args = argparse.Namespace()\n",
    "args.dataset = \"naver-toy\"\n",
    "args.dim=32\n",
    "args.n_hop=2\n",
    "args.kge_weight=0.01\n",
    "args.l2_weight=1e-7\n",
    "args.lr = 0.001\n",
    "args.batch_size = 256\n",
    "args.n_epoch = 100\n",
    "args.n_memory=32\n",
    "args.item_update_mode=\"plus_transform\"\n",
    "args.using_all_hops=True\n",
    "args.use_cuda = True\n",
    "args.show_topk =True\n",
    "args.gpu_id=0\n",
    "args.Ks=[20,40,60,80,100]\n",
    "args.test_flag = \"part\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = args.Ks\n",
    "BATCH_SIZE = args.batch_size\n",
    "batch_test_flag = True\n",
    "\n",
    "cores = multiprocessing.cpu_count() // 2\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading rating file ...\n",
      "splitting dataset ...\n",
      "reading KG file ...\n",
      "constructing knowledge graph ...\n",
      "constructing ripple set ...\n"
     ]
    }
   ],
   "source": [
    "data_info = load_data(args)\n",
    "show_loss = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_user, n_item, train_data, eval_data, test_data, n_entity, n_relation, ripple_set= data_info\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = {\n",
    "    \"n_users\" : n_user,\n",
    "    \"n_items\" : n_item,\n",
    "    \"n_entity\" : n_entity,\n",
    "    \"n_relation\" : n_relation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_set = _get_user_record(train_data, False)\n",
    "test_user_set = _get_user_record(test_data, False)\n",
    "user_dict = {\n",
    "    \"train_user_set\" : train_user_set,\n",
    "    \"test_user_set\" : test_user_set\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *\n",
    "from parser import parse_args\n",
    "\n",
    "import heapq\n",
    "\n",
    "\n",
    "def ranklist_by_heapq(user_pos_test, test_items, rating, Ks):\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "    # print(K_max_item_score)\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = 0.\n",
    "    return r, auc\n",
    "\n",
    "def get_auc(item_score, user_pos_test):\n",
    "    item_score = sorted(item_score.items(), key=lambda kv: kv[1])\n",
    "    item_score.reverse()\n",
    "    item_sort = [x[0] for x in item_score]\n",
    "    posterior = [x[1] for x in item_score]\n",
    "\n",
    "    r = []\n",
    "    for i in item_sort:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = AUC(ground_truth=r, prediction=posterior)\n",
    "    return auc\n",
    "\n",
    "def ranklist_by_sorted(user_pos_test, test_items, rating, Ks):\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = get_auc(item_score, user_pos_test)\n",
    "    return r, auc\n",
    "\n",
    "def get_performance(user_pos_test, r, auc, Ks):\n",
    "    precision, recall, ndcg, hit_ratio = [], [], [], []\n",
    "\n",
    "    for K in Ks:\n",
    "        precision.append(precision_at_k(r, K))\n",
    "        recall.append(recall_at_k(r, K, len(user_pos_test)))\n",
    "        ndcg.append(ndcg_at_k(r, K, user_pos_test))\n",
    "        hit_ratio.append(hit_at_k(r, K))\n",
    "\n",
    "    return {'recall': np.array(recall), 'precision': np.array(precision),\n",
    "            'ndcg': np.array(ndcg), 'hit_ratio': np.array(hit_ratio), 'auc': auc}\n",
    "\n",
    "\n",
    "def test_one_user(x):\n",
    "    # user u's ratings for user u\n",
    "    rating = x[0] # rate_batch\n",
    "    # uid\n",
    "    u = x[1]      # user_list_batch\n",
    "    # user u's items in the training set\n",
    "    try:\n",
    "        training_items = train_user_set[u]\n",
    "    except Exception:\n",
    "        training_items = []\n",
    "    # user u's items in the test set\n",
    "    user_pos_test = test_user_set[u]\n",
    "\n",
    "    all_items = set(range(0, n_items))\n",
    "\n",
    "    test_items = list(all_items - set(training_items))\n",
    "\n",
    "    if args.test_flag == 'part':\n",
    "        r, auc = ranklist_by_heapq(user_pos_test, test_items, rating, Ks)\n",
    "    else:\n",
    "        r, auc = ranklist_by_sorted(user_pos_test, test_items, rating, Ks)\n",
    "\n",
    "    return get_performance(user_pos_test, r, auc, Ks)\n",
    "\n",
    "def topk_settings(show_topk, train_data, test_data, n_item):\n",
    "    if show_topk:\n",
    "        user_num = 100\n",
    "        k_list = [1, 2, 5, 10, 20, 50, 100]\n",
    "        train_record = _get_user_record(train_data, True)\n",
    "        test_record = _get_user_record(test_data, False)\n",
    "        user_list = list(set(train_record.keys()) & set(test_record.keys()))\n",
    "        if len(user_list) > user_num:\n",
    "            user_list = np.random.choice(user_list, size=user_num, replace=False)\n",
    "        item_set = set(list(range(n_item)))\n",
    "        return user_list, train_record, test_record, item_set, k_list\n",
    "    else:\n",
    "        return [None] * 5\n",
    "\n",
    "\n",
    "def _show_info(recall_zip, title):\n",
    "    res = title+\": \"\n",
    "    for i,j in recall_zip:\n",
    "        res += \"K@%d:%.4f  \"%(i,j)\n",
    "    logging.info(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RippleNet(args, n_entity, n_relation)\n",
    "if args.use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    args.lr,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (534117491.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_360083/534117491.py\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    for user in test\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def topk_eval(args, model, train_data, test_data, n_item, n_user, ripple_set, batch_size):\n",
    "    u_batch_size = args.batch_size\n",
    "    i_batch_size = args.batch_size\n",
    "    user_list, train_record, test_record, item_set, k_list = topk_settings(show_topk=args.show_topk, \n",
    "                                                                            train_data=train_data, \n",
    "                                                                            test_data=test_data, \n",
    "                                                                            n_item=n_item)\n",
    "                                                                        \n",
    "    test_users = list(test_user_set.keys())\n",
    "    n_test_users = len(test_users)\n",
    "    n_user_batchs = n_test_users // u_batch_size + 1\n",
    "\n",
    "    count = 0\n",
    "    # test_item_list  = list(item_set - train_record[user])\n",
    "\n",
    "    test_users = list(test_record.keys())\n",
    "    n_user_batchs = n_test_users // u_batch_size + 1\n",
    "    count = 0\n",
    "    # for user in test_users:\n",
    "        # test_item_list = list(item_set - train_record[user])\n",
    "                                                                            \n",
    "    precision_list = {k:[] for k in k_list}\n",
    "    recall_list = {k:[] for k in k_list}\n",
    "    ndcg_list = {k:[] for k in k_list}\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    item_score_map = dict()\n",
    "    n_item_batchs = n_item // i_batch_size + 1\n",
    "    i_count = 0\n",
    "    for user in test_users:\n",
    "        for i_batch_id in range(n_item_batchs):\n",
    "            i_start = i_batch_id * i_batch_size\n",
    "            i_end = min((i_batch_id+1)*i_batch_size, n_item)\n",
    "\n",
    "            batch_test_data = test_data[i_start, i_end]\n",
    "            items, labels, memories_h, memories_r,memories_t = get_feed_dict(args, model, test_data, ripple_set, 0, args.batch_size)\n",
    "            return_dict = model(items, labels, memories_h, memories_r,memories_t)\n",
    "            scores = return_dict[\"scores\"]\n",
    "            \n",
    "            for item, score in zip(items, scores):\n",
    "                item_score_map[item] = score\n",
    "\n",
    "        item_score_pair_sorted = sorted(item_score_map.items(), key=lambda x : x[1], reverse=True)\n",
    "        item_sorted = [i[0] for i in item_score_pair_sorted]\n",
    "        for k in k_list:\n",
    "            hit_num = len(set(item_sorted[:k]) & set(test_record[user]))\n",
    "            recall_list[k].append(hit_num / len(set(test_record[user])))\n",
    "            precision_list[k].append(hit_num / k)\n",
    "    model.train()  \n",
    "    recall_k = [np.mean(recall_list[k]) for k in k_list]\n",
    "    precision_k = [np.mean(precision_list[k]) for k in k_list]\n",
    "    _show_info(zip(k_list, recall_k), \"recall\")\n",
    "    _show_info(zip(k_list, precision_k), \"precision\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_directory = f\"./training_log/{args.dataset}/\"\n",
    "if not os.path.exists(log_directory):\n",
    "    os.makedirs(log_directory)\n",
    "with open(log_directory+f\"RippleNet_{args.lr}.txt\",\"w\") as f:\n",
    "    for step in range(args.n_epoch):\n",
    "        # training\n",
    "        np.random.shuffle(train_data)\n",
    "        start = 0\n",
    "        train_s_t = time()\n",
    "        while start < train_data.shape[0]:\n",
    "            return_dict = model(*get_feed_dict(args, model, train_data, ripple_set, start, start + args.batch_size))\n",
    "            loss = return_dict[\"loss\"]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            start += args.batch_size\n",
    "            if show_loss:\n",
    "                print('%.1f%% %.4f' % (start / train_data.shape[0] * 100, loss.item()))\n",
    "        train_e_t = time()\n",
    "        ret = test(model, user_dict, n_params)\n",
    "        test_s_t = time()\n",
    "        \n",
    "        test_e_t = time()\n",
    "\n",
    "        test_res = PrettyTable()\n",
    "        test_res.field_names = [\"Epoch\", \"training time\", \"tesing time\", \"Loss\", \"recall\",  \"precision\", \"ndcg\"]\n",
    "        test_res.add_row(\n",
    "            [step, train_e_t-train_s_t, test_e_t-test_s_t, loss.item(), ret[\"recall\"], ret[\"precision\"], ret[\"ndcg\"]]\n",
    "        )\n",
    "        print(test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bdffd226db74f32b1e67dcb50a117797eaf84ad68693423287fabc603b29d02"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('tf2.5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
