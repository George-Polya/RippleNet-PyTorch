{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "from model import RippleNet\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "import multiprocessing\n",
    "from time import time\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from data_loader import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "args = argparse.Namespace()\n",
    "args.dataset = \"naver-toy\"\n",
    "args.dim=32\n",
    "args.n_hop=2\n",
    "args.kge_weight=0.01\n",
    "args.l2_weight=1e-7\n",
    "args.lr = 0.02\n",
    "args.batch_size = 256\n",
    "args.n_epoch = 10\n",
    "args.n_memory=32\n",
    "args.item_update_mode=\"plus_transform\"\n",
    "args.using_all_hops=True\n",
    "args.use_cuda = True\n",
    "args.show_topk =True\n",
    "args.gpu_id=0\n",
    "args.Ks=[20,40,60,80,100]\n",
    "args.test_flag = \"part\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = args.Ks\n",
    "BATCH_SIZE = args.batch_size\n",
    "batch_test_flag = True\n",
    "\n",
    "cores = multiprocessing.cpu_count() // 2\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading rating file ...\n",
      "splitting dataset ...\n",
      "reading KG file ...\n",
      "constructing knowledge graph ...\n",
      "constructing ripple set ...\n"
     ]
    }
   ],
   "source": [
    "data_info = load_data(args)\n",
    "show_loss = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_feed_dict, _get_user_record, _get_item_record\n",
    "from helper import early_stopping\n",
    "import heapq\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_info[0]\n",
    "eval_data = data_info[1]\n",
    "test_data = data_info[2]\n",
    "n_entity = data_info[3]\n",
    "n_relation = data_info[4]\n",
    "ripple_set = data_info[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.vstack([train_data, eval_data, train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_set = _get_user_record(all_data, True)\n",
    "all_item_set = _get_item_record(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(221122, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = len(all_user_set.keys())\n",
    "n_users = len(all_item_set.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cf(data):\n",
    "    def _get_user_record2(data):\n",
    "        user_history_dict = dict()\n",
    "        for rating in data:\n",
    "            user = rating[0]\n",
    "            item = rating[1]\n",
    "            label = rating[2]\n",
    "            if label == 1:\n",
    "                if user not in user_history_dict:\n",
    "                    user_history_dict[user] = set()\n",
    "                    # user_history_dict[user] = list()\n",
    "                user_history_dict[user].add(item)\n",
    "                # user_history_dict[user].append(item)\n",
    "        return user_history_dict\n",
    "    train_user_set2 = _get_user_record2(data)\n",
    "    arr = list()\n",
    "    for key in train_user_set2.keys():\n",
    "        for value in train_user_set2[key]:\n",
    "            arr.append(np.array([key, value]))\n",
    "    arr = np.array(arr)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cf = make_cf(train_data)\n",
    "test_cf = make_cf(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     1],\n",
       "       [    0,     2],\n",
       "       [    0,     5],\n",
       "       ...,\n",
       "       [13494,  7281],\n",
       "       [13496,  7283],\n",
       "       [13497,  7284]], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def remap_item(train_data, test_data):\n",
    "    # global n_users, n_items\n",
    "    train_user_dict = defaultdict(list)\n",
    "    test_user_dict = defaultdict(list)\n",
    "    n_users = max(max(train_data[:, 0]), max(test_data[:, 0])) + 1\n",
    "    n_items = max(max(train_data[:, 1]), max(test_data[:, 1])) + 1\n",
    "\n",
    "    for u_id, i_id in train_data:\n",
    "        train_user_dict[int(u_id)].append(int(i_id))\n",
    "    for u_id, i_id in test_data:\n",
    "        test_user_dict[int(u_id)].append(int(i_id))\n",
    "    return n_users, n_items, train_user_dict, test_user_dict\n",
    "\n",
    "n_users2, n_items2, train_user_dict, test_user_dict = remap_item(train_cf, test_cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7133"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13498"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7285"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_items2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12846"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7348"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ranklist_by_heapq(user_pos_test, test_items, rating, Ks):\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        print(f\"i: {i}, len(rating): {len(rating)}\")\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = 0.\n",
    "    return r, auc\n",
    "\n",
    "def get_auc(item_score, user_pos_test):\n",
    "    item_score = sorted(item_score.items(), key=lambda kv: kv[1])\n",
    "    item_score.reverse()\n",
    "    item_sort = [x[0] for x in item_score]\n",
    "    posterior = [x[1] for x in item_score]\n",
    "\n",
    "    r = []\n",
    "    for i in item_sort:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = AUC(ground_truth=r, prediction=posterior)\n",
    "    return auc\n",
    "\n",
    "def ranklist_by_sorted(user_pos_test, test_items, rating, Ks):\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = get_auc(item_score, user_pos_test)\n",
    "    return r, auc\n",
    "\n",
    "def get_performance(user_pos_test, r, auc, Ks):\n",
    "    precision, recall, ndcg, hit_ratio = [], [], [], []\n",
    "\n",
    "    for K in Ks:\n",
    "        precision.append(precision_at_k(r, K))\n",
    "        recall.append(recall_at_k(r, K, len(user_pos_test)))\n",
    "        ndcg.append(ndcg_at_k(r, K, user_pos_test))\n",
    "        hit_ratio.append(hit_at_k(r, K))\n",
    "\n",
    "    return {'recall': np.array(recall), 'precision': np.array(precision),\n",
    "            'ndcg': np.array(ndcg), 'hit_ratio': np.array(hit_ratio), 'auc': auc}\n",
    "\n",
    "\n",
    "def test_one_user(x):\n",
    "    # user u's ratings for user u\n",
    "    rating = x[0] # rate_batch\n",
    "    # uid\n",
    "    u = x[1]      # user_list_batch\n",
    "    # user u's items in the training set\n",
    "    try:\n",
    "        training_items = train_user_set[u]\n",
    "    except Exception:\n",
    "        training_items = []\n",
    "    # user u's items in the test set\n",
    "    user_pos_test = test_user_set[u]\n",
    "\n",
    "    # all_items = set(range(0, n_entity))\n",
    "    all_items = set(range(0, n_items))\n",
    "\n",
    "    test_items = list(all_items - set(training_items))\n",
    "\n",
    "    if args.test_flag == 'part':\n",
    "        r, auc = ranklist_by_heapq(user_pos_test, test_items, rating, Ks)\n",
    "    else:\n",
    "        r, auc = ranklist_by_sorted(user_pos_test, test_items, rating, Ks)\n",
    "\n",
    "    return get_performance(user_pos_test, r, auc, Ks)\n",
    "\n",
    "\n",
    "\n",
    "def test(args, model, data_info):\n",
    "    global Ks\n",
    "    Ks = args.Ks\n",
    "    \n",
    "    result = {'precision': np.zeros(len(Ks)),\n",
    "              'recall': np.zeros(len(Ks)),\n",
    "              'ndcg': np.zeros(len(Ks)),\n",
    "              'hit_ratio': np.zeros(len(Ks)),\n",
    "              'auc': 0.}\n",
    "    # args = parse_args()\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "    BATCH_SIZE = args.batch_size\n",
    "    batch_test_flag = args.batch_size\n",
    "\n",
    "    global n_entity\n",
    "    train_data = data_info[0]\n",
    "    eval_data = data_info[1]\n",
    "    test_data = data_info[2]\n",
    "    n_entity = data_info[3]\n",
    "    n_relation = data_info[4]\n",
    "    ripple_set = data_info[5]\n",
    "\n",
    "    train_cf = make_cf(train_data)\n",
    "    test_cf = make_cf(test_data)\n",
    "    global test_user_set, train_user_set\n",
    "    n_users, n_items, train_user_set, test_user_set = remap_item(train_cf, test_cf)\n",
    "\n",
    "    user_dict = {\n",
    "        \"train_user_set\":train_user_set,\n",
    "        \"test_user_set\":test_user_set\n",
    "    }\n",
    "\n",
    "    n_nodes = n_entity + n_users\n",
    "\n",
    "    \n",
    "    \n",
    "    # train_user_set = user_dict['train_user_set']\n",
    "    # test_user_set = user_dict['test_user_set']\n",
    "\n",
    "    pool = multiprocessing.Pool(cores)\n",
    "\n",
    "    u_batch_size = BATCH_SIZE\n",
    "    i_batch_size = BATCH_SIZE\n",
    "\n",
    "    test_users = list(test_user_set.keys())\n",
    "    n_test_users = len(test_users)\n",
    "    n_user_batchs = n_test_users // u_batch_size + 1\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "        \n",
    "    \n",
    "    for u_batch_id in range(n_user_batchs):\n",
    "        start = u_batch_id * u_batch_size\n",
    "        end = min((u_batch_id+1)*u_batch_size, n_test_users)\n",
    "\n",
    "        user_list_batch = test_users[start: end]\n",
    "        user_batch = torch.LongTensor(np.array(user_list_batch)).to(device)\n",
    "        user_emb = model.get_user_embeddings(*get_feed_dict(args, model, test_data, ripple_set, start, end))\n",
    "\n",
    "        if batch_test_flag:\n",
    "            # batch-item test\n",
    "            n_item_batchs = n_items // i_batch_size + 1\n",
    "            rate_batch = np.zeros(shape=(len(user_batch), n_items))\n",
    "\n",
    "            i_count = 0\n",
    "            for i_batch_id in range(n_item_batchs):\n",
    "                i_start = i_batch_id * i_batch_size\n",
    "                i_end = min((i_batch_id + 1) * i_batch_size, n_items)\n",
    "                # print(f\"i_start: {i_start}, i_end: {i_end}\")\n",
    "                item_batch = torch.LongTensor(np.array(range(i_start, i_end))).view(i_end-i_start).to(device)\n",
    "                item_emb = model.entity_emb(item_batch)\n",
    "\n",
    "                i_rate_batch = model.rating(user_emb, item_emb).detach().cpu()\n",
    "                # print(f\"rate_batch.shape: {rate_batch[:, i_start:i_end].shape}\")\n",
    "                # print(f\"i_rate_batch.shape: {i_rate_batch.shape}\")\n",
    "                rate_batch[:, i_start: i_end] = i_rate_batch\n",
    "                # print(f\"len(user_batch) : {len(user_batch)}\")\n",
    "                \n",
    "                i_count += i_rate_batch.shape[1]\n",
    "\n",
    "            assert i_count == n_items\n",
    "        else:\n",
    "            # all-item test\n",
    "            item_batch = torch.LongTensor(np.array(range(0, n_items))).view(n_items, -1).to(device)\n",
    "            \n",
    "            rate_batch = model.rating(user_emb, item_emb).detach().cpu()\n",
    "\n",
    "        user_batch_rating_uid = zip(rate_batch, user_list_batch)\n",
    "        batch_result = pool.map(test_one_user, user_batch_rating_uid)\n",
    "        count += len(batch_result)\n",
    "        for re in batch_result:\n",
    "            result['precision'] += re['precision']/n_test_users\n",
    "            result['recall'] += re['recall']/n_test_users\n",
    "            result['ndcg'] += re['ndcg']/n_test_users\n",
    "            result['hit_ratio'] += re['hit_ratio']/n_test_users\n",
    "            result['auc'] += re['auc']/n_test_users\n",
    "\n",
    "    assert count == n_test_users\n",
    "    pool.close()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_info[0]\n",
    "eval_data = data_info[1]\n",
    "test_data = data_info[2]\n",
    "n_entity = data_info[3]\n",
    "n_relation = data_info[4]\n",
    "ripple_set = data_info[5]\n",
    "\n",
    "model = RippleNet(args, n_entity, n_relation)\n",
    "if args.use_cuda:\n",
    "    model.cuda()\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    args.lr,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "cur_best_pre_0 = 0\n",
    "stopping_step = 0\n",
    "should_stop = False\n",
    "with open(f\"./training_log/RippleNet_{args.dataset}_{args.lr}.txt\",\"w\") as f:\n",
    "    for step in range(args.n_epoch):\n",
    "        # training\n",
    "        np.random.shuffle(train_data)\n",
    "        start = 0\n",
    "        train_s_t = time()\n",
    "        while start < train_data.shape[0]:\n",
    "            return_dict = model(*get_feed_dict(args, model, train_data, ripple_set, start, start + args.batch_size))\n",
    "            loss = return_dict[\"loss\"]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            start += args.batch_size\n",
    "            if show_loss:\n",
    "                print('%.1f%% %.4f' % (start / train_data.shape[0] * 100, loss.item()))\n",
    "        train_e_t = time()\n",
    "        # evaluation\n",
    "\n",
    "        test_s_t = time()\n",
    "        ret = test(args, model, data_info)\n",
    "        test_e_t = time()\n",
    "        result_table = PrettyTable()\n",
    "        result_table.field_names = [\"Epoch\", \"training time\", \"tesing time\", \"Loss\", \"recall\", \"ndcg\", \"precision\", \"hit_ratio\"]\n",
    "        result_table.add_row(\n",
    "            [step, train_e_t - train_s_t, test_e_t - test_s_t, loss.item(), ret['recall'], ret['ndcg'], ret['precision'], ret['hit_ratio']]\n",
    "        )\n",
    "\n",
    "        # cur_best_pre_0, stopping_step, should_stop = early_stopping(ret['recall'][0], cur_best_pre_0,\n",
    "        #                                                                     stopping_step, expected_order='acc',\n",
    "        #                                                                     flag_step=10)\n",
    "        if ret['recall'][0] == cur_best_pre_0 and args.save:\n",
    "\n",
    "            torch.save(model.state_dict(), args.out_dir + 'model_' + args.dataset + '.ckpt')\n",
    "        print(result_table)\n",
    "        f.write(str(result_table)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bdffd226db74f32b1e67dcb50a117797eaf84ad68693423287fabc603b29d02"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('tf2.5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
