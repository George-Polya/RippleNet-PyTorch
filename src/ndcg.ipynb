{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "from data_loader import load_data\n",
    "import logging\n",
    "np.random.seed(2022)\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--dataset DATASET] [--dim DIM]\n",
      "                             [--n_hop N_HOP] [--kge_weight KGE_WEIGHT]\n",
      "                             [--l2_weight L2_WEIGHT] [--lr LR]\n",
      "                             [--batch_size BATCH_SIZE] [--n_epoch N_EPOCH]\n",
      "                             [--n_memory N_MEMORY]\n",
      "                             [--item_update_mode ITEM_UPDATE_MODE]\n",
      "                             [--using_all_hops USING_ALL_HOPS]\n",
      "                             [--show_topk SHOW_TOPK] [--gpu_id GPU_ID]\n",
      "                             [--Ks [KS]] [--test_flag [TEST_FLAG]]\n",
      "                             [--use_cuda USE_CUDA] [--save SAVE]\n",
      "                             [--out_dir OUT_DIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9008 --control=9006 --hb=9005 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"7b4191c7-eaf4-4d8d-99e2-2d0919d637ef\" --shell=9007 --transport=\"tcp\" --iopub=9009 --f=/tmp/tmp-189298ok1TtuGjdngU.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "from model import RippleNet\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from train import train, _get_user_record, get_feed_dict\n",
    "\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.dataset = \"naver-toy\"\n",
    "args.dim=32\n",
    "args.n_hop=2\n",
    "args.kge_weight=0.01\n",
    "args.l2_weight=1e-7\n",
    "args.lr = 0.02\n",
    "args.batch_size = 256\n",
    "args.n_epoch = 10\n",
    "args.n_memory=32\n",
    "args.item_update_mode=\"plus_transform\"\n",
    "args.using_all_hops=True\n",
    "args.use_cuda = True\n",
    "args.show_topk =True\n",
    "args.gpu_id=0\n",
    "args.cuda=True\n",
    "args.Ks=[20,40,60,80,100]\n",
    "args.test_flag = \"part\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = args.Ks\n",
    "BATCH_SIZE = args.batch_size\n",
    "batch_test_flag = True\n",
    "cores = multiprocessing.cpu_count() // 2\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info = load_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_info[0]\n",
    "eval_data = data_info[1]\n",
    "test_data = data_info[2]\n",
    "n_entity = data_info[3]\n",
    "n_relation = data_info[4]\n",
    "ripple_set = data_info[5]\n",
    "\n",
    "model = RippleNet(args, n_entity, n_relation)\n",
    "if args.use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    args.lr,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in tqdm(range(args.n_epoch)):\n",
    "    np.random.shuffle(train_data)\n",
    "    start = 0\n",
    "    return_dict = model(*get_feed_dict(args, model, train_data, ripple_set, start, start + args.batch_size))\n",
    "    loss = return_dict[\"loss\"]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start =0\n",
    "items, labels, memories_h, memories_r, memories_t = get_feed_dict(args, model, train_data, ripple_set, start, start + args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memories_t[0].shape # (batch_size, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_set = _get_user_record(train_data,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user_set = _get_user_record(test_data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dict = {\n",
    "    \"train_user_set\" : train_user_set,\n",
    "    \"test_user_set\" : test_user_set\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_user_set.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_user_set.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.vstack([train_data, test_data,eval_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_item_record(data):\n",
    "    item_dict = dict()\n",
    "    for rating in data:\n",
    "        user = rating[0]\n",
    "        item = rating[1]\n",
    "        label = rating[2]\n",
    "\n",
    "        if label == 1:\n",
    "            if item not in item_dict:\n",
    "                item_dict[item] = set()\n",
    "            item_dict[item].add(user)\n",
    "\n",
    "    return item_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_item_set = _get_item_record(all_data)\n",
    "all_user_set = _get_user_record(all_data, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_item_set.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = len(all_user_set.keys())\n",
    "n_users = len(all_item_set.keys())\n",
    "n_nodes = n_entity + n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params ={\n",
    "    \"n_items\" : n_items,\n",
    "    \"n_users\" : n_users,\n",
    "    \"n_entities\" : n_entity,\n",
    "    \"n_relations\" : n_relation,\n",
    "    \"n_nodes\" : n_nodes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.o_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *\n",
    "# from .parser import parse_args\n",
    "import multiprocessing\n",
    "import heapq\n",
    "from time import time\n",
    "\n",
    "def ranklist_by_heapq(user_pos_test, test_items, rating, Ks):\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = 0.\n",
    "    return r, auc\n",
    "\n",
    "def get_auc(item_score, user_pos_test):\n",
    "    item_score = sorted(item_score.items(), key=lambda kv: kv[1])\n",
    "    item_score.reverse()\n",
    "    item_sort = [x[0] for x in item_score]\n",
    "    posterior = [x[1] for x in item_score]\n",
    "\n",
    "    r = []\n",
    "    for i in item_sort:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = AUC(ground_truth=r, prediction=posterior)\n",
    "    return auc\n",
    "\n",
    "def ranklist_by_sorted(user_pos_test, test_items, rating, Ks):\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = get_auc(item_score, user_pos_test)\n",
    "    return r, auc\n",
    "\n",
    "def get_performance(user_pos_test, r, auc, Ks):\n",
    "    precision, recall, ndcg, hit_ratio = [], [], [], []\n",
    "\n",
    "    for K in Ks:\n",
    "        precision.append(precision_at_k(r, K))\n",
    "        recall.append(recall_at_k(r, K, len(user_pos_test)))\n",
    "        ndcg.append(ndcg_at_k(r, K, user_pos_test))\n",
    "        hit_ratio.append(hit_at_k(r, K))\n",
    "\n",
    "    return {'recall': np.array(recall), 'precision': np.array(precision),\n",
    "            'ndcg': np.array(ndcg), 'hit_ratio': np.array(hit_ratio), 'auc': auc}\n",
    "\n",
    "\n",
    "def test_one_user(x):\n",
    "    test_flag = args.test_flag\n",
    "    # user u's ratings for user u\n",
    "    rating = x[0] # rate_batch\n",
    "    \n",
    "    # uid\n",
    "    u = x[1]\n",
    "    # user u's items in the training set\n",
    "    try:\n",
    "        training_items = train_user_set[u]\n",
    "    except Exception:\n",
    "        training_items = []\n",
    "    # user u's items in the test set\n",
    "    user_pos_test = test_user_set[u]\n",
    "    # print(user_pos_test)\n",
    "    all_items = set(range(0, n_entity))\n",
    "\n",
    "    test_items = list(all_items - set(training_items))\n",
    "    # print(len(test_items))\n",
    "    if test_flag == 'part':\n",
    "        r, auc = ranklist_by_heapq(user_pos_test, test_items, rating, Ks)\n",
    "    else:\n",
    "        r, auc = ranklist_by_sorted(user_pos_test, test_items, rating, Ks)\n",
    "\n",
    "    return get_performance(user_pos_test, r, auc, Ks)\n",
    "\n",
    "# def test(model, user_dict, n_params):\n",
    "#     result = {'precision': np.zeros(len(Ks)),\n",
    "#               'recall': np.zeros(len(Ks)),\n",
    "#               'ndcg': np.zeros(len(Ks)),\n",
    "#               'hit_ratio': np.zeros(len(Ks)),\n",
    "#               'auc': 0.}\n",
    "\n",
    "#     global n_users, n_items\n",
    "#     n_items = n_params['n_items']\n",
    "#     n_users = n_params['n_users']\n",
    "\n",
    "#     global train_user_set, test_user_set\n",
    "#     train_user_set = user_dict['train_user_set']\n",
    "#     test_user_set = user_dict['test_user_set']\n",
    "\n",
    "#     pool = multiprocessing.Pool(cores)\n",
    "\n",
    "#     u_batch_size = BATCH_SIZE\n",
    "#     i_batch_size = BATCH_SIZE\n",
    "\n",
    "#     test_users = list(test_user_set.keys())\n",
    "#     n_test_users = len(test_users)\n",
    "#     n_user_batchs = n_test_users // u_batch_size + 1\n",
    "\n",
    "#     count = 0\n",
    "\n",
    "    \n",
    "    \n",
    "#     for u_batch_id in range(n_user_batchs):\n",
    "#         start = u_batch_id * u_batch_size\n",
    "#         end = (u_batch_id + 1) * u_batch_size\n",
    "\n",
    "#         user_list_batch = test_users[start: end]\n",
    "#         user_batch = torch.LongTensor(np.array(user_list_batch)).to(device)\n",
    "#         u_g_embeddings = model.get_user_embeddings()\n",
    "\n",
    "#         if batch_test_flag:\n",
    "#             # batch-item test\n",
    "#             n_item_batchs = n_items // i_batch_size + 1\n",
    "#             rate_batch = np.zeros(shape=(len(user_batch), n_items))\n",
    "\n",
    "#             i_count = 0\n",
    "#             for i_batch_id in range(n_item_batchs):\n",
    "#                 i_start = i_batch_id * i_batch_size\n",
    "#                 i_end = min((i_batch_id + 1) * i_batch_size, n_items)\n",
    "\n",
    "#                 item_batch = torch.LongTensor(np.array(range(i_start, i_end))).view(i_end-i_start).to(device)\n",
    "#                 i_g_embddings = model.entity_emb(item_batch)\n",
    "\n",
    "#                 i_rate_batch = model.rating(u_g_embeddings, i_g_embddings).detach().cpu()\n",
    "\n",
    "#                 rate_batch[:, i_start: i_end] = i_rate_batch\n",
    "#                 i_count += i_rate_batch.shape[1]\n",
    "\n",
    "#             assert i_count == n_items\n",
    "#         else:\n",
    "#             # all-item test\n",
    "#             item_batch = torch.LongTensor(np.array(range(0, n_items))).view(n_items, -1).to(device)\n",
    "#             # i_g_embddings = entity_gcn_emb[item_batch]\n",
    "#             # rate_batch = model.rating(u_g_embeddings, i_g_embddings).detach().cpu()\n",
    "#             rate_batch = model.rating(u_g_embeddings, i_g_embddings).detach().cpu()\n",
    "\n",
    "#         user_batch_rating_uid = zip(rate_batch, user_list_batch)\n",
    "#         batch_result = pool.map(test_one_user, user_batch_rating_uid)\n",
    "#         count += len(batch_result)\n",
    "\n",
    "#         for re in batch_result:\n",
    "#             result['precision'] += re['precision']/n_test_users\n",
    "#             result['recall'] += re['recall']/n_test_users\n",
    "#             result['ndcg'] += re['ndcg']/n_test_users\n",
    "#             result['hit_ratio'] += re['hit_ratio']/n_test_users\n",
    "#             result['auc'] += re['auc']/n_test_users\n",
    "\n",
    "#     assert count == n_test_users\n",
    "#     pool.close()\n",
    "#     return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_user_set[8169])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'precision': np.zeros(len(Ks)),\n",
    "              'recall': np.zeros(len(Ks)),\n",
    "              'ndcg': np.zeros(len(Ks)),\n",
    "              'hit_ratio': np.zeros(len(Ks)),\n",
    "              'auc': 0.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = n_params[\"n_items\"]\n",
    "n_users = n_params[\"n_users\"]\n",
    "\n",
    "train_user_set = user_dict[\"train_user_set\"]\n",
    "test_user_set = user_dict[\"test_user_set\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool(cores)\n",
    "u_batch_size = BATCH_SIZE\n",
    "i_batch_size = BATCH_SIZE\n",
    "\n",
    "test_users = list(test_user_set.keys())\n",
    "n_test_users = len(test_users)\n",
    "n_user_batchs = n_test_users // u_batch_size +1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11032 - (11032 // batch_size) * batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_count = 0\n",
    "for u_batch_id in range(n_user_batchs):\n",
    "    start = u_batch_id * u_batch_size\n",
    "    end = min((u_batch_id+1)*u_batch_size, n_test_users)\n",
    "\n",
    "    user_list_batch = test_users[start:end]\n",
    "    user_batch = torch.LongTensor(np.array(user_list_batch)).to(device)\n",
    "    user_emb = model.get_user_embeddings(*get_feed_dict(args, model, test_data, ripple_set, start, end))\n",
    "    \n",
    "    n_items_batch = n_entity // i_batch_size + 1\n",
    "    rate_batch = np.zeros(shape=(len(user_batch), n_entity))\n",
    "\n",
    "    i_count = 0\n",
    "\n",
    "    for i_batch_id in range(n_items_batch):\n",
    "        i_start = i_batch_id * i_batch_size\n",
    "        i_end = min((i_batch_id+1)*i_batch_size, n_entity)\n",
    "\n",
    "        item_batch = torch.LongTensor(np.array(range(i_start, i_end))).view(i_end-i_start).to(device)\n",
    "        item_emb = model.entity_emb(item_batch)\n",
    "\n",
    "        i_rate_batch = model.rating(user_emb, item_emb).detach().cpu()\n",
    "        rate_batch[:, i_start:i_end] = i_rate_batch\n",
    "        # print(f\"rate_batch.shape: {rate_batch[:, i_start:i_end].shape}\")\n",
    "        # print(f\"i_rate_batch.shape: {i_rate_batch.shape}\")\n",
    "        print(f\"len(user_batch) : {len(user_batch)}\")\n",
    "    # print(rate_batch.shape, len(user_list_batch))\n",
    "    user_batch_rating_uid = zip(rate_batch, user_list_batch)\n",
    "    # batch_result = list()\n",
    "    batch_result = pool.map(test_one_user, user_batch_rating_uid)\n",
    "    for re in batch_result:\n",
    "\n",
    "        result['precision'] += re['precision']/n_test_users\n",
    "        result['recall'] += re['recall']/n_test_users\n",
    "        result['ndcg'] += re['ndcg']/n_test_users\n",
    "        result['hit_ratio'] += re['hit_ratio']/n_test_users\n",
    "        result['auc'] += re['auc']/n_test_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_user_batchs * 256 + 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bdffd226db74f32b1e67dcb50a117797eaf84ad68693423287fabc603b29d02"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('tf2.5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
